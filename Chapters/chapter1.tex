\chapter{Introduction}

\section{Basic inequalities and theorems}


%% --- Markov's inequality
\begin{theorem}[Markov's inequality]\label{markov}
  For a random variable $X$ with $\P\{X < 0\} = 0$ and $t>0$, we have
  \[ \P\{X \geq t\} \leq \frac{\E X}{t}.\]
\end{theorem}

\vspace*{1em}

\begin{proof}
 In the first place, note that
 \[\arraycolsep=2pt
  \begin{array}{rrrll}
  X & = & X\cdot \1_{\{X \geq t\}} & + & X \cdot \1_{\{X < t\}}\\
    & \geq &t \cdot \1_{\{X \geq t\}} & + & 0,
 \end{array}\]
and thus,
\[ \E X \geq t \cdot \E \1_{\{X \geq t\}} = t \cdot \P\{X \geq t\}. \]
\end{proof}
%% --------------------

\vspace*{2em}

%% --- Chebyshev's inequality
\begin{theorem}[Chebyshev's inequality]\label{chebyshev}
  For $t > 0$, a random variable $X$ with mean $\mu = \E X$ and variance $\sigma^2 = \Var X$, we have
  \[ \P\{|X-\mu| \geq t\} \leq \frac{\sigma^2}{t^2}. \] 
\end{theorem}

\begin{proof}
  We apply Markov's inequality to the non-negative random variable $Y = |X-\mu|^2$ in order to obtain the desired result
  \[ \P\{|X-\mu| \geq t \} = \P\{|X-\mu|^2 \geq t^2 \} \leq \frac{\E [{(X-\mu)}^2]}{t^2} = \frac{\sigma^2}{t^2}.\] 
\end{proof}
%% --------------------

% \vspace*{2em}

% %% --- Jensen's inequality
% \begin{theorem}[Jensen's inequality]\label{jensen}
%   For any real valued random variable $X$ and convex function $\varphi$
%   \[ \varphi(\E X) \leq \E \varphi(X). \] 
% \end{theorem}

% \begin{proof}[]
  
% \end{proof}
% %% --------------------


\vspace*{2em}

\section{Why bother?}

The concentration inequalities are used to obtain information on how a random variable is distributed at some specific places of its domain. In the most common scenarios, these inequalities will be used to quantify how concentrated a random variable is around its center. In other words, how fast the probability decays as we move towards the tails. For example,

\[ \P\{|X-\mu| \geq t\} < f(t) << 1. \]

A concentration inequality is specially useful when this probability cannot be calculated at a low computational cost or estimated with high precision. The following  will illustrate a case where using concentration inequalities achieves the best results.

\subsection{Coin Tossings}

A coin tossing game is fair if the chances of winning are equal to the chances of losing. We can verify from a sample of $N$ games that the game is not rigged if the number of heads in the sample is not very distant from the average $N/2$. However, there's a chance that one may classify the coin as rigged, even when the coin is fair. By the \textit{Law of Large Numbers}, we know that the larger the sample, the less likely it is to obtain a false positive. But let's ask ourselves how fast this probability converges to 0.

\vspace*{1em}

Let $S_N \sim \Bi(N,1/2)$ denote the number of heads in a fair coin tossing game. Then,
\[ \mu = \E S_N = \frac{N}{2}, \hspace*{2em} \sigma^2 = \Var S_N = \frac{N}{4}. \] 
For a fixed $\varepsilon > 0$, we may classify a coin tossing game as rigged if, after $N$ trials, the ratio of success falls outside the interval $[0, \tfrac{1+\varepsilon}{2}]$
\[ S_N \geq \mu +  \frac{\varepsilon}{2} N = \frac{1+\varepsilon}{2} N. \]

It's clear that calculating the exact probability of the previous event for any $N,\; \varepsilon$ is a very demanding task computationally. The Chebyshev's inequality~\ref{chebyshev} gives us a ``good-enough'' result for this problem,

\[ \P\left\{S_N \geq \mu +  \frac{\varepsilon}{2} N\right\} \leq 
\P\left\{|S_N-\mu| \geq  \frac{\varepsilon}{2} N\right\} \leq \sigma^2 \frac{4}{\varepsilon^2 N^2} = \frac{1}{\varepsilon^2 N}.\] 

Therefore, the probability of bad events tends to 0 at least linearly with the number of games.

\subsection{Central Limit Theorem}

The proof of the following three theorems can be found in~\cite{boucheron2003concentration}

%% --- Central Limit Theorem
\begin{theorem}\label{centrallimit}
    Let $X_i$ be a i.i.d.~sample. Let $S_N = \sum_{i = 1}^N X_i$, with mean $\mu = \E S_N$ and variance $\sigma^2 = \Var S_N$. If  
    \[ Z_N =  \frac{S_N - N\cdot \E X_i}{\sqrt{N \cdot \Var X_i}} =  \frac{S_N - \mu}{\sqrt{N}\sigma},\] 
    then,
    \[ Z_N \to Z \sim \mathcal{N}(0,1),\hbox{ in distribution.} \] 

    \hfill $\square$
\end{theorem}
%% --------------------

\vspace*{2em}

%% --- Normal Tails
\begin{theorem}[Tails of the Normal Distribution]\label{normaltails}
    Let $Z\sim \mathcal{N}(0,1)$, for $t>0$ we have
    \[ \left( \frac{1}{t}-\frac{1}{t^3}\right) \frac{1}{\sqrt{2\pi}} \exp\left(\frac{-t^2}{2}\right) \leq \P\{Z \geq t\} \leq \frac{1}{t}\;\frac{1}{\sqrt{2\pi}} \exp\left(\frac{-t^2}{2}\right). \]

    \hfill $\square$
\end{theorem}
%% --------------------

With that in mind, we might naively assume that better bounds can be obtained by using the previous theorem. For a large enough $N$ we can say that for the coin tossing,
\[ Z_N = \frac{S_N- N/2}{\sqrt{N/4}} \] 
\[ \implies \P \left\{S_N \geq \frac{1+\varepsilon}{2} N \right\} = \P\left\{Z_N \geq \varepsilon \sqrt{N} \right\} \sim \P\left\{Z \geq \varepsilon \sqrt{N} \right\}.\]
However, this raises the question of whether we can draw the following conclusion from Theorem~\ref{normaltails}:
\[ \P \left\{S_N \geq \frac{1+\varepsilon}{2} N \right\} \leq \frac{1}{\varepsilon \sqrt{N}}\;\frac{1}{\sqrt{2\pi}} \exp\left(\frac{-\varepsilon^2 \cdot N}{2}\right). \]
Unfortunately, the answer is no. The following theorem will show why.


%% --- Berry-Essen CLT
\begin{theorem}[Convergence Rate for Central Limit Theorem]\label{berryessen}
  For $Z_N$, $Z$ in Theorem~\ref{centrallimit}, we have:
  \[ \left|\P\{Z_N \geq t\} - \P\{Z \geq t\}\right| = O(\tfrac{1}{\sqrt{N}}). \]
  \hfill $\square$
\end{theorem}
%% --------------------

Since the approximation error of the Central Limit Theorem is of greater order than the normal bounds, the previous results cannot be taken into account.

\vspace*{1em}

In the context of coin tossing, this may not matter at all because the linear bound obtained using Chebyshev's inequality indicates that the probability of wrongly classifying a fair coin as a rigged coin converges to zero. Even the Central Limit Theorem shows, in a less precise way, this convergence. However, for some specific problems in statistics, these basic tools are not precise enough to solve them. The main objective of this project is to study different ideas that improve these bounds and show examples where they can be used.

\section{Cantelli's inequality}
We can start with a small modification of the Chebyshev's bound for the one-sided tails

%% --- Cantelli's inequality
\begin{theorem}[Cantelli's Inequality]\label{cantelli}
  For $t > 0$, a random variable $X$ with mean $\mu = \E X$ and variance $\sigma^2 = \Var X$, we have
  \[ \P\{X-\mu \geq t\} \leq \frac{\sigma^2}{t^2 + \sigma^2}. \] 
\end{theorem}

\begin{proof}
  In the first place note that,
  \[ \P\{ Y \geq s\} \leq \P\{ Y \geq s\} + \P\{ Y \leq s\} = \P\{|Y| \geq s\} = \P\{ Y^2 \geq s^2\} \tag*{($\star$)}. \] 
  Let $u \geq 0$, define $Y = X-\mu + u$ and $s = t+u$ to obtain
  \[ \P\{X-\mu \geq t\} = \P\{X-\mu + u \geq t + u\} = \P\{Y \geq s\}. \]
  We use $(\star)$ and Markov's inequality~(\ref{markov}) on $Y^2$ to conclude,
  \[ \P\{Y \geq s\} \overset{(\star)}{\leq} \P\{ Y^2 \geq s^2\} \overset{(\ref{markov})}{\leq} \frac{\E[{(X-\mu+u)}^2]}{{(t+u)}^2}.\]
  By linearity of expectation,
  \[ \E[{(X-\mu+u)}^2] = \E[{(X-\mu)}^2] + 2u \cdot \underbrace{\E(X-\mu)}_{0} + E(u^2) = \sigma^2 + u^2. \]

  Finally, we choose an optimal $u = \frac{\sigma^2}{t}$ to conclude
  \[
    \P\{X-\mu \geq t\} \leq \frac{\sigma^2 + u^2}{{(t+u)}^2}
     = \frac{\sigma^2 + \sigma^4/t^2}{{{(t+\sigma^2/t)}^2}}
     = \frac{\sigma^2 (\tfrac{t^2+\sigma^2}{t^2})}{{(\tfrac{t^2+\sigma^2}{t})}^2}
     = \frac{\sigma^2}{t^2+\sigma^2}.\]
\end{proof}
%% --------------------

On the other hand, the two-sided tail inequality, Cantelli's inequality is not always better than Chebyshev,

\begin{corollary}[Two-sided Cantelli inequality]
  \[\P\{|X-\mu| \geq t\} \leq \frac{2\sigma^2}{t^2 + \sigma^2}. \] 
\end{corollary}

In fact, this bound is only better than Chebyshev's $t^2+\sigma^2 \geq 2t^2$, or equivalently, when $\sigma^2 \geq t^2$. However, in this case both formulas provide bounds greater than 1, and thus, are useless. Therefore, the conclusion is that, in general, Chebyshev's inequality is better for two-sided tails and Cantelli is for one-sided tails.