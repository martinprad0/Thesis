\chapter{Introduction}

\section{Basic inequalities and theorems}


%% --- Markov's inequality
\begin{theorem}[Markov's inequality]\label{markov}
  For a random variable $X$ with $\P\{X < 0\} = 0$ and $t>0$, we have
  \[ \P\{X \geq t\} \leq \frac{\E X}{t}.\]
  It follows that for a non-decreasing function $\varphi$ which only takes non-negative values,
  \[ \P\{X \geq t\} = \P\{\varphi(X) \geq \varphi(t)\} \leq \frac{\E\varphi(X)}{\varphi(t)}.\]
\end{theorem}

\vspace*{1em}

\begin{proof}
 In the first place, note that
 \[\arraycolsep=2pt
  \begin{array}{rrrll}
  X & = & X\cdot \1_{\{X \geq t\}} & + & X \cdot \1_{\{X < t\}}\\
    & \geq &t \cdot \1_{\{X \geq t\}} & + & 0,
 \end{array}\]
and thus,
\[ \E X \geq t \cdot \E \1_{\{X \geq t\}} = t \cdot \P\{X \geq t\}. \]
For the second statement, apply the same argument on the random variable $Y := \varphi(X)$ and the constant $s := \varphi(t)$.
\end{proof}
%% --------------------

\vspace*{2em}

%% --- Chebyshev's inequality
\begin{theorem}[Chebyshev's inequality]\label{chebyshev}
  For $t > 0$ and a random variable $X$ with mean $\mu = \E X$ and variance $\sigma^2 = \Var X$, then
  \[ \P\{|X-\mu| \geq t\} \leq \sigma^2 t^{-2}. \] 
\end{theorem}

\begin{proof}
  Applying Markov's inequality with $\varphi: x \mapsto x^2$ we obtain,
  \[ \P\{|X-\mu| \geq t \} = \P\{|X-\mu|^2 \geq t^2 \} \leq \frac{\E [{(X-\mu)}^2]}{t^2} = \sigma^2 t^{-2}.\] 
\end{proof}
%% --------------------

\vspace*{2em}

%% --- Jensen's inequality
\begin{theorem}[Jensen's inequality]\label{jensen}
  For any real valued random variable $X$ and convex function $\varphi$
  \[ \varphi(\E X) \leq \E \varphi(X). \] 
\end{theorem}

\begin{proof}[]
  
\end{proof}
%% --------------------


\vspace*{2em}

\section{Why bother?}

The concentration inequalities are used to obtain information on how a random variable is distributed at some specific places of its domain. In the most common scenarios, these inequalities will be used to quantify how concentrated a random variable at its tails, for example,

\[ \P\{|X-\mu| \geq t\} < f(t) << 1. \]

A concentration inequality is specially useful when this probability cannot be calculated at a low computational cost or estimated with high precision. The following  will illustrate a case where using concentration inequalities achieves the best results.

\subsection{Coin Tossing}

A coin tossing game is fair if the chances of winning are equal to the chances of losing. We can verify from a sample of $N$ games that the game is not rigged if the number of heads in the sample is not very distant from the average $N/2$. However, there's a chance that one may classify the coin as rigged, even when the coin is fair. By the \textit{Law of Large Numbers}, we know that the larger the sample, the less likely it is to obtain a false positive. But let's ask ourselves how fast this probability converges to 0.

\vspace*{1em}

Let $S_N \sim \Bi(N,1/2)$ denote the number of heads in a fair coin tossing game. Then,
\[ \mu = \E S_N = \frac{N}{2}, \hspace*{2em} \sigma^2 = \Var S_N = \frac{N}{4}. \] 
For a fixed $\varepsilon > 0$, we may classify a coin tossing game as rigged if, after $N$ trials, the ratio of heads vs tails in the sample is greater than $[1+\varepsilon:1-\varepsilon]$, or similarly,
\[ S_N \geq \mu +  \frac{\varepsilon}{2} N = \frac{1+\varepsilon}{2} N. \]

Using the Chebyshev inequality~\ref{chebyshev}, we assert that

\[ \P\left\{S_N \geq \mu +  \frac{\varepsilon}{2} N\right\} \leq 
\P\left\{|S_N-\mu| \geq  \frac{\varepsilon}{2} N\right\} \leq \sigma^2 \frac{4}{\varepsilon^2 N^2} = \frac{1}{\varepsilon^2 N}.\] 

Therefore, the probability of bad events tends to 0 at least linearly with the number of games.

\subsection{Central Limit Theorem}

The proof of the following theorems can be found in (ref)

%% --- Central Limit Theorem
\begin{theorem}\label{centrallimit}
    Let $X_i$ be a i.i.d.~sample. Let $S_N = \sum_{i = 1}^N X_i$, with mean $\mu = \E S_N$ and variance $\sigma^2 = \Var S_N$. If  
    \[ Z_N =  \frac{S_N - N\cdot \E X_i}{\sqrt{N \cdot \Var X_i}} =  \frac{S_N - \mu}{\sqrt{N}\sigma},\] 
    then,
    \[ Z_N \to Z \sim \mathcal{N}(0,1),\hbox{ in distribution.} \] 

    \hfill $\square$
\end{theorem}
%% --------------------

\vspace*{2em}

%% --- Central Limit Theorem
\begin{theorem}[Tails of the Normal Distribution]\label{normaltails}
    Let $Z\sim \mathcal{N}(0,1)$, for $t>0$ we have
    \[ \left( \frac{1}{t}-\frac{1}{t^3}\right) \frac{1}{\sqrt{2\pi}} \exp\left(\frac{-t^2}{2}\right) \leq \P\{Z \geq t\} \leq \frac{1}{t}\;\frac{1}{\sqrt{2\pi}} \exp\left(\frac{-t^2}{2}\right). \]

    \hfill $\square$
\end{theorem}
%% --------------------

With that in mind, we might naively assume that better bounds can be obtained by using the previous theorem. For a large enough $N$ we can say that for the coin tossing,
\[ Z_N = \frac{S_N- N/2}{\sqrt{N/4}} \] 
\[ \implies \P \left\{S_N \geq \frac{1+\varepsilon}{2} N \right\} = \P\left\{Z_N \geq \varepsilon \sqrt{N} \right\} \sim \P\left\{Z \geq \varepsilon \sqrt{N} \right\}.\]
However, this raises the question of whether we can draw the following conclusion from Theorem~\ref{normaltails}:
\[ \P \left\{S_N \geq \frac{1+\varepsilon}{2} N \right\} \leq \frac{1}{\varepsilon \sqrt{N}}\;\frac{1}{\sqrt{2\pi}} \exp\left(\frac{-\varepsilon^2 \cdot N}{2}\right). \]
Unfortunately, the answer is no. The following theorem will show why.


%% --- Berry-Essen CLT
\begin{theorem}[Convergence Rate for Central Limit Theorem]\label{berryessen}
  For $Z_N$, $Z$ in Theorem~\ref{centrallimit}, we have:
  \[ \left|\P\{Z_N \geq t\} - \P\{Z \geq t\}\right| = O(\tfrac{1}{\sqrt{N}}). \]
  \hfill $\square$
\end{theorem}
%% --------------------

Since the approximation error is greater than the bound, the previous results cannot be taken into account.

In the context of coin tossing, this may not matter at all because the linear bound obtained using Chebyshev's inequality indicates that the probability of wrongly classifying a fair coin as a rigged coin converges at least linearly to zero. Even the Central Limit Theorem shows in a less precise way this convergence. However, for some specific problems in statistics, these basic tools are not precise enough to solve them. In the following chapters, we will show some examples were better crafted strategies are needed in order to get bounds to the tails of the random variables.