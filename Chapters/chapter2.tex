\chapter{Exponential Inequalities}

Even if we are satisfied with the linear convergence rate provided by Chebyshev's inequality or the improvement of one sided tails given by Cantelli's inequality, there is a simple but powerful modification we can make to Markov's inequality that will greatly improve both bounds. The following result will provide the main idea from which most of the exponential inequalities are derived.

%% --- MGF Inequality
\begin{theorem}[MGF inequality]\label{mgf}
  Let $X_i$ be a finite sequence of independent random variables and let $S_N := \sum_{i = 1}^N a_i X_i$. Let $\lambda > 0$. The following inequality holds,
  \[ \P\left\{  S_N \geq t\right\} \leq e^{-\lambda t}\cdot \prod_{i = 1}^N \E e^{\lambda a_i X_i}. \] 

\end{theorem}

\begin{proof}
  Let $\lambda > 0$, using Markov's inequality (Theorem~\ref{markov}) we assert that since $x\mapsto e^{\lambda x}$ is a non-decreasing function,
  \[ \P\left\{  S_N \geq t\right\} = \P\left\{  e^{\lambda S_N} \geq e^{\lambda t}\right\} \leq e^{-\lambda t} \cdot \E \exp\left( \lambda \sum_{i = 1}^N a_i X_i \right). \]
  Since $X_i$ are independent, the MGF of $S_N$ is the product of MGFs of each $X_i$:
  \[\E \exp\left( \lambda \sum_{i = 1}^N a_i X_i \right) = \prod_{i = 1}^N \E e^{\lambda a_i X_i} \] 
  \[ \implies \P\left\{  S_N \geq t\right\} \leq e^{-\lambda t}\cdot \prod_{i = 1}^N \E e^{\lambda a_i X_i}. \]
\end{proof}
%% --------------------

The following two theorems are examples on how we can obtain even tighter bounds than the ones we've already studied. In particular, these theorems can be obtained from the previous theorem and are considered, by some authors, as corollaries of the previous result.

%% --- Chernoff's inequality
\begin{theorem}[Chernoff's inequality]\label{chernoff:bernoulli}
  Let $X_i \sim \Be(p_i)$ be independent random variables. Define $S_N = \sum_{i = 1}^{N} X_i$ and let $\mu = \E S_N$. Then, for $t > \mu$, we have
  
  \[ \P\left\{  S_N \geq t\right\} \leq  {\left(\frac{\mu}{t}\right)}^t e^{-\mu+t} .\] 
\end{theorem}

\begin{proof}
  In the first place, use Theorem~\ref{mgf} to assert that for a $\lambda > 0$ that
  \[\P\left\{  S_N \geq t\right\} \leq e^{-\lambda t}\cdot \prod_{i = 1}^N \E e^{\lambda X_i}.\] 
  Now it is left to bound every $X_i$ individually. Using the inequality $1+x \leq e^x$ we obtain
  \[ \E e^{\lambda X_i} = e^{\lambda}p_i + (1-p_i) = 1 + (e^{\lambda}-1)p_i  \leq \exp(e^{\lambda}-1)e^{p_i}.\]
  Finally, we plug this inequality on the equation to conclude that
  \[e^{-\lambda t}\cdot \prod_{i = 1}^N \E e^{\lambda X_i} \leq e^{-\lambda t}\cdot \prod_{i = 1}^N \exp((e^{\lambda}-1) p_i) = e^{-\lambda t} \exp((e^{\lambda}-1) \mu). \]
  By using the substitution $\lambda = \ln (t/\mu)$ we obtain the desired result,
  \[ \P\left\{  S_N \geq t\right\} \leq {\left(\frac{\mu}{t}\right)}^t \exp{\left(\frac{\mu t}{\mu}-\mu\right)} = {\left(\frac{\mu}{t}\right)}^t e^{-\mu+t}. \]
\end{proof}
%% --------------------

Another exponential inequality that is derived using a similar technique is Hoeffding's inequality:

%% --- Hoeffding's inequality
\begin{theorem}[Hoeffding's inequality]\label{hoeffding:bounded}
  Let $X_1, \ldots, X_N$ be independent random variables, such that $X_i \in [a_i,b_i]$ for every $i = 1,\ldots,N$. Define $S_N = \sum_{i = 1}^{N} X_i$ and let $\mu = \E S_N$. Then, for every $t > 0$, we have
  \[ \P\left\{  S_N \geq \mu + t\right\} \leq \exp\left( \frac{-2t^2}{\sum{( a_i - b_i )}^2}\right). \]
\end{theorem}

\begin{proof}
  Since, for $\lambda > 0$, $x \mapsto e^{\lambda x}$ is a convex function, it follows that, for any bounded random variable $X \in [a,b]$:
  \[ e^{\lambda X} \leq \frac{e^{\lambda a}(b-X)}{b-a} + \frac{e^{\lambda b}(X-a)}{b-a},\hspace*{1em} a \leq b. \]
  Then, take expectations on both sides of the equation to obtain:
  \[ \E e^{\lambda X} \leq \frac{(b-\E X) \cdot e^{\lambda a}}{b-a} + \frac{(\E X -a) \cdot e^{\lambda b}}{b-a}. \]
  To simplify the expression, let $\alpha = (\E X -a)/(b-a)$, $\beta = (b-\E X)/(b-a)$ and $u = \lambda (b-a)$. Since $a < \E X < b$, it follows that $\alpha$ and $\beta$ are positive. Also, note that,
  \[ \alpha + \beta = \frac{\E X-a}{b-a} + \frac{b-\E X}{b-a} = \frac{b-a}{b-a} = 1. \] 
  Now,
  \[ \ln \E e^{\lambda X} \leq \ln (\beta e^{-\alpha u} + \alpha e^{\beta u}) = -\alpha u + \ln(\beta + \alpha e^{u}). \] 
  This function is differentiable with respect to $u$.
  \[\everymath{\displaystyle}\arraycolsep=1.8pt\def\arraystretch{1.8}
    \begin{array}{rcl}
    L(u) & = & -\alpha u + \ln(\beta + \alpha e^{u}),\\
    L'(u) & = & -\alpha + \frac{\alpha}{\alpha + \beta e^{-u}},\\
    L''(u) & = & \frac{\alpha}{\alpha + \beta e^{-u}} \cdot \frac{\beta e^{-u}}{\alpha + \beta e^{-u}}.
  \end{array} \]
  Note that if $x = \frac{\alpha}{\alpha + \beta e^{-u}} \leq 1$, then $L''(u) = x(1-x) \leq \frac{1}{4}$. Remember that $\alpha + \beta = 1$. Now, by expanding the Taylor series we obtain,
  \begin{equation}
    \tag{$\star$} \everymath{\displaystyle}\arraycolsep=1.8pt\def\arraystretch{2}
  \begin{array}{rcl}
    L(u) & = & L(0) + uL'(0) + \frac{1}{2} u^2 L''(u)\\
    & = &\ln(\beta + \alpha) + u \left(-\alpha + \frac{\alpha}{\alpha + \beta}\right) + \frac{1}{2} u^2 L''(u) \\
    & = & \frac{1}{2} u^2 L''(u)\\
    &\leq & \frac{1}{8}\lambda^2 {(b-a)}^2.
  \end{array}
  \end{equation}

  Finally, use the inequality from Theorem~\ref{mgf} to conclude that
  \[\everymath{\displaystyle}\arraycolsep=1.8pt\def\arraystretch{2}
    \begin{array}{rcl}
      \P\{S_N-\mu \geq t\} & \leq & e^{-\lambda t} \prod_{i = 1}^N \E e^{\lambda X_i} \\
      &\leq^{(\star)} & e^{-\lambda t} \exp\left(\frac{1}{8}\lambda^2 \sum_{i = 1}^{N} {(b_i-a_i)}^2\right).
    \end{array}\] 

  Use the substitution $\lambda = 4t {\left(\sum_{i=1}^N{(b_i-a_i)}^2\right)}^{-1}$ to get the desired result.

\end{proof}
%% --------------------



%% --- Hoeffding's inequality (Bernoulli)
\begin{corollary}\label{hoeffding:bernoulli}
  Let $X_1,\ldots, X_N$ be independent random Bernoulli variables such that $X_i \sim \Be(p_i)$, then
  \[ \P\left\{ \sum_{i = 1}^N (X_i - p_i) \geq t\right\} \leq \exp\left( \frac{-2t^2}{N}\right). \] 
\end{corollary}

\begin{proof}[]

\end{proof}
%% --------------------

Returning to the coin tossing problem, we can now make a stronger assertion of the rate of convergence of a false negative classification using Hoeffding inequality:
\[\P \left\{S_N- \frac{N}{2} \geq \frac{\varepsilon}{2} N \right\} \leq \exp\left( - \varepsilon^2 N \right). \] 

% However, this raises the question of which of the previous inequalities is better for a given problem. In the previous case, we chose Hoeffding's inequality, but when dealing with any specific problem, one needs to determine the criteria for deciding whether it's more appropriate to use Chernoff, Hoeffding, or any other inequality. In the following section, we will try to identify situations where one of these inequalities is more suitable than the other.



\section{Uniform Law of Large Numbers}
For any probability measure $P$ on the real line and $t \in \R$, define $P_n$ as the empirical probability measure obtained from an independent sample $X_1, \ldots, X_n$ of $P$, that is:
\[ P_n(t) = P_n(-\infty, t) = n^{-1} \cdot \sum_{i = 1}^n \1_{\{ X_i \leq t\}}.\]

From the law of large numbers we know that for a fixed $t$, $P_n(t)$ converges to $P(t)$ with probability 1. However we can formulate a stronger statement on this convergence. The first application of concentration inequalities we are going to explore is the uniform law of large numbers, which states the following:

\begin{theorem}[Glivenko-Cantelli Theorem]\label{glivenko-cantelli}
  For $P,\; P_n$ and $t \in \R$,
  \[ \|P_n - P\| = \sup_{t\in \Q} \left|P_n(t) - P(t)\right| \overset{p}{\longrightarrow} 0.\]  
\end{theorem} 

\begin{proof}
  The proof, adapted from~\cite{pollard1984convergence}, consists of 5 steps. At first instance, the author clarifies that we can impose the condition $t\in \Q$ to avoid problems with measurability. The author later proves that the theorem is true if $t$ is allowed to vary in $\R$, but for practical purposes, we will only prove it for rationals. Another remark the author makes is that this result from the real line can be later generalized for some classes of ``polynomial discrimination'', and we will cover more about this in the final section.

  \subsubsection*{First Symmetrization}
  In the first place, define $P_n'$ as the empirical measure obtained from an independent but identical sample $X_1',\ldots, X_n'$ of $P$. Note that for any fixed $t$, $P_n(t)$ and $P_n'(t)$ are random variables derived from their respective samples which satisfy that
  \[ \E P_n(t) = \E P_n'(t) = P(t). \] 
  We will bound the concentration of $\|P_n- P_n'\|$ first, which will later result in a bound for $\|P_n - P\|$ at the end of the following lemma.

  \vspace*{1em}

  For now, fix a value for $\varepsilon > 0$, and keep in mind that $Z = P_n - P$, $Z' = P_n' - P$, $\alpha = \frac{1}{2}\varepsilon$ and $\beta = \frac{1}{2}$. Also, for this case define $\AA = \{(-\infty, t) : t\in \R\}$

  \begin{lemma}\label{gc:l1}
    Let ${\{Z(A)\}}_{A\in \AA}$ and ${\{Z'(A)\}}_{A\in \AA}$ be independent and identical functions defined over the same collection of sets $\AA$. Also, assume that there exist $\alpha, \beta > 0$ such that
    \[ \P\left\{|Z(A)| \leq \alpha \right\} \geq \beta, \hspace*{1em} \forall A \in \AA \]
    It follows that, for any $\varepsilon > 0$,
    \[  \P\left\{\sup_{A\in \AA} |Z(A)| > \varepsilon \right\} \leq \beta^{-1} \P\left\{\sup_{A\in \AA} |Z(A)-Z'(A)| > \varepsilon - \alpha \right\}. \]     
  \end{lemma}

  \begin{proof}
    Fix an index $B$ in the set $T_\varepsilon = \{ A \in \AA \;:\; |Z(A)| > \varepsilon\}$.
    it follows from the independence of $Z$ and $Z'$ that,

    \[ \beta \cdot \P \{ T_\varepsilon \neq \emptyset\} \leq \P \{ T_\varepsilon \neq \emptyset \text{ and }  |Z'(B)|\leq \alpha \} \] 

    Now, if $T_\varepsilon \neq \emptyset \text{ and }  |Z'(B)|\leq \alpha  $, then $|Z'(B)| \leq \alpha\text{ and } |Z(B)| > \varepsilon$. Thus, since $\sup_{A\in \AA} |Z(A)| > \varepsilon$ implies $T_\varepsilon \neq \emptyset$, it follows that

    \[\everymath{\displaystyle}\arraycolsep=1.8pt\def\arraystretch{1.8}
    \begin{array}{rcl}
      \beta \cdot \P\left\{\sup_{A\in \AA} |Z(A)| > \varepsilon \right\} & \leq & \P\{|Z'(B)| \leq \alpha\text{ and } |Z(B)| > \varepsilon\}\\
       & \leq & \P\{ |Z(B) - Z'(B)| > \varepsilon - \alpha \}\\
        & \leq & \P\left\{\sup_{A\in \AA} |Z(A)-Z'(A)| > \varepsilon - \alpha \right\}.
    \end{array} \] 
  \end{proof}
  Using Chevyshev's inequality (\ref{chebyshev}) we know that the hypothesis is satisfied for the values of $\alpha$ and $\beta$ we chose:
  \[\forall t\in \R: \P\left\{ |Z'(t)| \leq \alpha \right\} = \P\{|P_n(t) - P(t)| \leq \varepsilon\} \geq \frac{1}{2} = \beta,\hspace*{1em} \text{if $n \geq 8\varepsilon^{-2}$}.\]
  Therefore, using the previous lemma, we conclude that
  \begin{equation}
    \label{gc:1} 
    \P\{\|P_n-P\| > \varepsilon\} \leq 2 \P\{\|P_n-P_n'\| > \tfrac{1}{2} \varepsilon\},\hspace*{1em} \text{if $n \geq 8\varepsilon^{-2}$}.
  \end{equation}

  \vspace*{1em}

  \subsubsection*{Second Symmetrization}
  The following trick will allow us to stop considering all of the $2n$ data points from the previous symmetrization, and will help us to create a simpler random variable. We will initially prove the trick for unidimensional random variables, but in chapter 4, we will generalize this proof for any kind of set on $\R^n$.

  \begin{lemma}\label{gc:l2}
    Let $\sigma_1, \ldots, \sigma_n$ be Rademacher i.i.d.~random variables, that is $\P\{\sigma_i = 1\} = \P\{\sigma_i = -1\} = 1/2$, and they are independent from $X_i$ and $X_i'$. Let $Y_i = \1_{\{ X_i' \in A\}} - \1_{\{ X_i \in A\}}$, and note that,
    \[ \P\{Y_i = x\} = \P\{\sigma_i Y_i = x\},\hspace*{1em} x \in \{-1,0,1\}. \] 
  \end{lemma}

  \begin{proof} In the first place, since $X_i$ and $X'_i$ are two independent and identical copies of the same distribution, the following equality holds: 

    \[ \begin{array}{rcl}
      \P\{Y_i = 1\} & = & \P\{X_i \in A\} \P\{X'_i \not\in A\}\\
      & = &  \P\{X'_i \in A\} \P\{X_i \not\in A\}\\
      & = & \P\{Y_i = -1\}.
    \end{array} \]
  On the other hand, since $\sigma_i$ is also independent of $Y_i$, it follows that
  \[ \everymath{\displaystyle}\arraycolsep=1.8pt\def\arraystretch{1.8}
  \begin{array}{rcl}
    \P\{\sigma_i Y_i = 1\} & = & \P\{Y_i = 1, \sigma_i = 1\} + \P\{Y_i = -1, \sigma_i = -1\}\\
    & = & \P\{Y_i = 1\} \P\{\sigma_i = 1\} + \P\{Y_i = -1\} \P\{\sigma_i = 1\}\\
    & = & \frac{1}{2} \P\{Y_i = 1\} + \frac{1}{2} \P\{Y_i = 1\}\\
    & = & \P\{Y_i = 1\} = \P\{Y_i = -1\} = \P\{\sigma_i Y_i = -1\}.
  \end{array}
    \]
    Thus,
    \[  \P\{\sigma_i Y_i = \pm 1\} = \P\{Y_i = \pm 1\},\hspace*{1em}  \P\{\sigma_i Y_i = 0\} = \P\{ Y_i = 0\}. \] 
  \end{proof}
  
  It follows that since $P_n - P_n' = n^{-1} \sum_{i \leq n} Y_i$,
  \begin{equation}
    \label{gc:2} 
    \everymath{\displaystyle}\arraycolsep=1.8pt\def\arraystretch{3}
  \begin{array}{rcl}
    \P\{\|P_n - P_n'\| > \tfrac{1}{2}\varepsilon\} & = & \P\left\{\sup_{t\in \Q} \left|n^{-1}\sum_{i = 1}^n \sigma_i Y_i\right| >\tfrac{1}{2}\varepsilon \right\}\\
    & \leq & \P\left\{\sup_{t\in \Q} \left|n^{-1}\sum_{i = 1}^n \sigma_i\1_{\{ X_i < t\}}\right| >\tfrac{1}{4}\varepsilon \right\}\\
    & & + \P\left\{\sup_{t\in \Q} \left|n^{-1}\sum_{i = 1}^n \sigma_i\1_{\{ X_i' < t\}}\right| >\tfrac{1}{4}\varepsilon \right\}
  \end{array}
  \end{equation}
  \[ = 2\P\{\|P_n^\circ\| > \tfrac{1}{4} \varepsilon\}.  \] 
  where $P_n^\circ = n^{-1}\sum_{i\leq n} \sigma_i \1_{\{ X_i < t\}}$. Then, from equations $\ref{gc:1},\;\ref{gc:2}$ we conclude that for $n \geq 8\varepsilon^{-2}$,
  \[ \P\{\|P_n-P\| > \varepsilon\} \leq 4 \P\{\|P_n^\circ\|> \tfrac{1}{4}\varepsilon\}.\]

  \vspace*{1em}

  \subsubsection*{Maximal Inequality}

  \[\begin{tikzcd}
    {-\infty} & {X_{(1)}} & {X_{(2)}} & {X_{(3)}} & \cdots & {X_{(n)}} & \infty
    \arrow["{t_1}", no head, from=1-2, to=1-3]% chktex 18
    \arrow["{t_2}", no head, from=1-3, to=1-4]% chktex 18
    \arrow["{t_3}", no head, from=1-4, to=1-5]% chktex 18
    \arrow["{t_{n-1}}", no head, from=1-5, to=1-6]% chktex 18
    \arrow["{t_0}"', from=1-2, to=1-1]% chktex 18
    \arrow["{t_n}", from=1-6, to=1-7]% chktex 18
  \end{tikzcd}\]

  For any given sample $X = X_1, \ldots, X_n$, define $X_{(j)}$ as the $j$-th observation when we order the observations, and fix $t_j \in (X_{(j)}, X_{(j+1)}]$ for every $j\leq n$ as the picture above shows. Note that if $t \in (X_{(j)}, X_{(j+1)}]$, then $P_n^\circ (t) = P_n^\circ (t_j)$ because:  % chktex 9

  \[ \everymath{\displaystyle}\arraycolsep=1.8pt\def\arraystretch{1.8}
  \begin{array}{rclcl}
    P_n^\circ (t) & = & n^{-1}\sum_{i = 1}^n \sigma_i\1_{\{ X_i < t\}}, & & \hfill t \in (X_{(j)}, X_{(j+1)}] \\ % chktex 9
    & = & n^{-1}\sum_{i = j+1}^{n} \sigma_i\1_{\{ X_{(i)} < t\}} & + & n^{-1} \sum_{i = 1}^{j} \sigma_i \1_{\{ X_{(i)} < t\}}\\
    & = & n^{-1}\sum_{i = j+1}^{n} \sigma_i \cdot 1 & + &  \hfill0\hfill\\
    & = & P_n^\circ (t_j).
  \end{array}
  \]

  It follows that for some $k$, $\|P_n^\circ\| = |P_n^\circ (t_k)|$, and thus,
   \begin{equation} \label{gc:3}
    \everymath{\displaystyle}\arraycolsep=1.4pt\def\arraystretch{1.5}
    \begin{array}{rcl}
      \P\{ \|P_n^\circ\| > \tfrac{1}{4} \varepsilon \;|\; X \} & \leq & \sum_{j = 0}^n \max_j \P\{ |P_n^\circ(t_j)| > \tfrac{1}{4} \varepsilon \;|\; X \}\\
      & \leq & (n+1) \cdot \P\{ |P_n^\circ(t_k)| > \tfrac{1}{4} \varepsilon \;|\; X \} .
    \end{array} 
  \end{equation}
  \vspace*{1em}

  \subsubsection*{Exponential Bounds}

  Since for any given sample, $\sigma \1_{X_i < t} \in [-1,1]$, we can use Hoeffding's Inequality~\ref{hoeffding:bounded} to obtain the following inequality
  \[ \P \{ |P_n^\circ(A)| > \tfrac{1}{4} \varepsilon \} \leq 2 \exp\left( \frac{-2{(n\varepsilon/4)}^2}{4n}\right) = 2e^{-n\varepsilon^2/32}, \hspace*{1em} \forall A \in \AA.\]
  We use equation $\ref{gc:3}$ to conclude
  \begin{equation} \label{gc:4}
    \P\{ \|P_n^\circ\| > \tfrac{1}{4} \varepsilon \;|\; X\} \leq 2(n+1) e^{-n\varepsilon^2/32}.
  \end{equation} 

  \vspace*{1em}

  \subsubsection*{Integration}

  Finally, applying the formula $P\{A\} = \E_X[\P\{A | X\}]$, we conclude that

  \begin{equation}
    \label{gc:5}\begin{array}{rcl}
      \P \{\|P_n-P\| > \varepsilon\} & = & \E [\P \{\|P_n-P\| > \varepsilon \;|\; X\}]\\
      & \leq & \E [8(n+1) e^{-n\varepsilon^2/32}]\\
      & = & 8(n+1) e^{-n\varepsilon^2/32}.
    \end{array} 
  \end{equation}

  The Borel-Cantelli Lemma states that if the probability of a sequence of events is summable, that is \(\sum_{n = 1}^\infty \P\{E_n\} < \infty\), then
  \begin{equation}\label{borel-cantelli}
    \limsup_n \P(E_n) = \P \left\{ \bigcap_{n=1}^{\infty}\bigcup_{k = n}^{\infty} E_n\right\} = 0.
  \end{equation}

  Since the inequality we obtain through the previous steps is exponential, the probabilities of the events $E_n = \{\|P_n-P\| > \varepsilon\}$ are summable:

  \[  \sum_{n = 1}^\infty \P \{\|P_n-P\| > \varepsilon\} < \infty .\] 

  Therefore, using the Borel-Cantelli lemma we conclude that

  \[ \P \{\|P_n-P\| > \varepsilon\} \to 0 \mbox{ with probability 1.} \] 
\end{proof}

In chapter 4 we will elaborate further on the details required to transform this powerful theorem in a more general version.

% chktex 17