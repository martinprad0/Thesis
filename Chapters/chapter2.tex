\chapter{Exponential Inequalities}

Even if we are satisfied with the linear convergence rate provided by Chebyshev's inequality, there are simple ways to improve this bound. The following result will provide the idea from which the exponential inequalities derive

%% --- MGF Inequality
\begin{theorem}[MGF inequality]\label{mgf}
  Let $X_i$ be a finite sequence of independent random variables and let $S_N := \sum_{i = 1}^N a_i X_i$. Let $\lambda > 0$ the following inequality holds,
  \[ \P\left\{  S_N \geq t\right\} \leq e^{-\lambda t}\cdot \prod_{i = 1}^N \E e^{\lambda a_i X_i} \] 

\end{theorem}

\begin{proof}
  Let $\lambda > 0$, using Markov's inequality (Theorem~\ref{markov}) we assert that since $x\mapsto e^{\lambda x}$ is a non-decreasing function,
  \[ \P\left\{  S_N \geq t\right\} = \P\left\{  e^{\lambda S_N} \geq e^{\lambda t}\right\} \leq e^{-\lambda t} \cdot \E \exp\left( \lambda \sum_{i = 1}^N a_i X_i \right). \]
  Since $X_i$ are independent, the MGF of $S_N$ is the product of MGFs of each $X_i$:
  \[\E \exp\left( \lambda \sum_{i = 1}^N a_i X_i \right) = \prod_{i = 1}^N \E e^{\lambda a_i X_i} \] 
  \[ \implies \P\left\{  S_N \geq t\right\} \leq e^{-\lambda t}\cdot \prod_{i = 1}^N \E e^{\lambda a_i X_i}. \]
\end{proof}
%% --------------------

The following two theorems are examples on how we can obtain tighter bounds than the ones provided by Chebyshev's inequality. In particular, these theorems are derived from the idea of the previous theorem and are considered as corollaries by some authors.

%% --- Chernoff's inequality
\begin{theorem}[Chernoff's inequality]\label{chernoff:bernoulli}
  Let $X_i \sim \Be(p_i)$ be independent random variables. Define $S_N = \sum_{i = 1}^{N} X_i$ and let $\mu = \E S_N$. Then, for $t > \mu$, we have
  
  \[ \P\left\{  S_N \geq t\right\} \leq  {\left(\frac{\mu}{t}\right)}^t e^{-\mu+t} .\] 
\end{theorem}

\begin{proof}
  In the first place, use Theorem~\ref{mgf} to assert that for a $\lambda > 0$ that
  \[\P\left\{  S_N \geq t\right\} \leq e^{-\lambda t}\cdot \prod_{i = 1}^N \E e^{\lambda X_i} \] 
  Now it is left to bound every $X_i$ individually. Using the inequality $1+x \leq e^x$ we obtain
  \[ \E e^{\lambda X_i} = e^{\lambda}p_i + (1-p_i) = 1 + (e^{\lambda}-1)p_i  \leq \exp(e^{\lambda}-1)e^{p_i}.\]
  Finally, we plug this inequality on the equation to conclude that
  \[e^{-\lambda t}\cdot \prod_{i = 1}^N \E e^{\lambda X_i} \leq e^{-\lambda t}\cdot \prod_{i = 1}^N \exp((e^{\lambda}-1) p_i) = e^{-\lambda t} \exp((e^{\lambda}-1) \mu). \]
  By using the substitution $\lambda = \ln (t/\mu)$ we obtain the desired result,
  \[ \P\left\{  S_N \geq t\right\} \leq {\left(\frac{\mu}{t}\right)}^t \exp{\left(\frac{\mu t}{\mu}-\mu\right)} = {\left(\frac{\mu}{t}\right)}^t e^{-\mu+t}. \]
\end{proof}
%% --------------------

Another exponential inequality that is derived using a similar technique is Hoeffding's inequality:

%% --- Hoeffding's inequality
\begin{theorem}[Hoeffding's inequality]\label{hoeffding:bounded}
  Let $X_1, \ldots, X_N$ be independent random variables, such that $X_i \in [a_i,b_i]$ for every $i = 1,\ldots,N$. Define $S_N = \sum_{i = 1}^{N} X_i$ and let $\mu = \E S_N$. Then, for every $t > 0$, we have
  \[ \P\left\{  S_N \geq \mu + t\right\} \leq \exp\left( \frac{-2t^2}{\sum{( a_i - b_i )}^2}\right). \]
\end{theorem}

\begin{proof}
  Since $x \mapsto e^x$ is a convex function, it follows that, for a random variable $X \in [a,b]$:
  \[ e^{\lambda X} \leq \frac{e^{\lambda a}(b-X)}{b-a} + \frac{e^{\lambda b}(X-a)}{b-a},\hspace*{1em} a \leq b. \]
  Next, take expectations on both hands of the equation to obtain:
  \[ \E e^{tX} \leq \frac{(b-\E X) \cdot e^{\lambda a}}{b-a} - \frac{(\E X -a) \cdot e^{\lambda b}}{b-a}. \]
  To simplify the expression, let $\alpha = (\E X -a)/(b-a)$, $\beta = (b-\E X)/(b-a)$ and $u = \lambda (b-a)$. Since $a < \E X < b$, it follows that $\alpha$ and $\beta$ are positive. Also, note that,
  \[ \alpha + \beta = \frac{\E X-a}{b-a} + \frac{b-\E X}{b-a} = \frac{b-a}{b-a} = 1. \] 
  Now,
  \[ \ln \E e^{\lambda X} \leq \ln (\beta e^{-\alpha u} + \alpha e^{\beta u}) = -\alpha u + \ln(\beta + \alpha e^{u}). \] 
  This function is differentiable with respect to $u$.
  \[\everymath{\displaystyle}\arraycolsep=1.8pt\def\arraystretch{1.8}
    \begin{array}{rcl}
    L(u) & = & -\alpha u + \ln(\beta + \alpha e^{u})\\
    L'(u) & = & -\alpha + \frac{\alpha}{\alpha + \beta e^{-u}}\\
    L''(u) & = & \frac{\alpha}{\alpha + \beta e^{-u}} \cdot \frac{\beta e^{-u}}{\alpha + \beta e^{-u}}.
  \end{array} \]
  Note that if $x = \frac{\alpha}{\alpha + \beta e^{-u}} \leq 1$, then $L''(u) = x(1-x) \leq \frac{1}{4}$. Remember that $\alpha + \beta = 1$. Now, by expanding the Taylor series we obtain,
  \begin{equation}
    \tag{$\star$} \everymath{\displaystyle}\arraycolsep=1.8pt\def\arraystretch{2}
  \begin{array}{rcl}
    L(u) & = & L(0) + uL'(0) + \frac{1}{2} u^2 L''(u)\\
    & = &\ln(\beta + \alpha) + u \left(-\alpha + \frac{\alpha}{\alpha + \beta}\right) + \frac{1}{2} u^2 L''(u) \\
    & = & \frac{1}{2} u^2 L''(u)\\
    &\leq & \frac{1}{8}\lambda^2 {(b-a)}^2.
  \end{array}
  \end{equation}

  Finally, use the inequality from Theorem~\ref{mgf} to conclude that
  \[\everymath{\displaystyle}\arraycolsep=1.8pt\def\arraystretch{2}
    \begin{array}{rcl}
      \P\{S_N-\mu \geq t\} & \leq & e^{-\lambda t} \prod_{i = 1}^N \E e^{\lambda X_i} \\
      &\leq^{(\star)} & e^{-\lambda t} \exp\left(\frac{1}{8}t^2 \sum_{i = 1}^{N} {(b_i-a_i)}^2\right)
    \end{array}\]  

\end{proof}
%% --------------------



%% --- Hoeffding's inequality (Bernoulli)
\begin{corollary}\label{hoeffding:bernoulli}
  Let $X_1,\ldots, X_N$ be independent random Bernoulli variables such that $X_i \sim \Be(p_i)$, then
  \[ \P\left\{ \sum_{i = 1}^N (X_i - p_i) \geq t\right\} \leq \exp\left( \frac{-2t^2}{N}\right). \] 
\end{corollary}

\begin{proof}[]

\end{proof}
%% --------------------

Returning to the coin tossing problem, we can now make a stronger assertion of the rate of convergence of a false negative classification using Hoeffding inequality:
\[\P \left\{S_N- \frac{N}{2} \geq \frac{\varepsilon}{2} N \right\} \leq \exp\left( - \varepsilon N \right). \] 

However, this raises the question of which of the previous inequalities is better for a given problem. In the previous case, we chose Hoeffding's inequality, but when dealing with any specific problem, one needs to determine the criteria for deciding whether it's more appropriate to use Chernoff, Hoeffding, or any other inequality. In the following section, we will try to identify situations where one of these inequalities is more suitable than the other.

\section{Which inequality is better?}
Let's start with a small improvement of the Chebyshev's bound for the one-sided tails

%% --- Cantelli's inequality
\begin{theorem}[Cantelli's Inequality]\label{cantelli}
  For $t > 0$, a random variable $X$ with mean $\mu = \E X$ and variance $\sigma^2 = \Var X$, we have
  \[ \P\{X-\mu \geq t\} \leq \frac{\sigma^2}{t^2 + \sigma^2}. \] 
\end{theorem}

\begin{proof}
  In the first place note that,
  \[ \P\{ Y \geq s\} \leq \P\{ Y \geq s\} + \P\{ Y \leq s\} = \P\{|Y| \geq s\} = \P\{ Y^2 \geq s^2\} \tag*{($\star$)}. \] 
  Let $u \geq 0$, define $Y = X-\mu + u$ and $s = t+u$ to obtain
  \[ \P\{X-\mu \geq t\} = \P\{X-\mu + u \geq t + u\} = \P\{Y \geq s\}. \]
  We use $(\star)$ and Markov's inequality~(\ref{markov}) on $Y^2$ to conclude,
  \[ \P\{Y \geq s\} \overset{(\star)}{\leq} \P\{ Y^2 \geq s^2\} \overset{(\ref{markov})}{\leq} \frac{\E[{(X-\mu+u)}^2]}{{(t+u)}^2}.\]
  By linearity of expectation,
  \[ \E[{(X-\mu+u)}^2] = \E[{(X-\mu)}^2] + 2u \cdot \underbrace{\E(X-\mu)}_{0} + E(u^2) = \sigma^2 + u^2. \]

  Finally, we choose an optimal $u = \frac{\sigma^2}{t}$ to conclude
  \[
    \P\{X-\mu \geq t\} \leq \frac{\sigma^2 + u^2}{{(t+u)}^2}
     = \frac{\sigma^2 + \sigma^4/t^2}{{{(t+\sigma^2/t)}^2}}
     = \frac{\sigma^2 (\tfrac{t^2+\sigma^2}{t^2})}{{(\tfrac{t^2+\sigma^2}{t})}^2}
     = \frac{\sigma^2}{t^2+\sigma^2}\]
\end{proof}
%% --------------------

On the other hand, the two-sided tail inequality, Cantelli's inequality is not always better than Chebyshev,

\begin{corollary}[Two-sided Cantelli inequality]
  \[\P\{|X-\mu| \geq t\} \leq \frac{2\sigma^2}{t^2 + \sigma^2}. \] 
\end{corollary}

In fact, this bound is only better than Chebyshev's $t^2+\sigma^2 \leq 2t^2$, or equivalently, when $\sigma^2 \leq t^2$. However, in this case both inequalities give bounds greater than 1, and thus, are useless. Therefore, we conclude that in general Chebyshev's is better for two-sided tails and Cantelli's for one-sided tails.


\section{Uniform Law of Large Numbers}

For any probability measure $P$ on the real line and $t \in \R$, define $P_n$ as the empirical probability measure obtain from an independent sample $X_1, \ldots, X_n$ of $P$, that is:
\[ P_n(t) = n^{-1} \cdot \sum_{i = 1}^n \1_{\{ X_i < t\}}.\]

From the law of large numbers we know that for a fixed $t$, $P_n(t)$ converges to $P(t)$ with probability 1. However we can formulate a stronger statement on this convergence. The first application of concentration inequalities we are going to explore is the uniform law of large numbers, which states the following:

\begin{theorem}[Glivenko-Cantelli Theorem]\label{glivenko-cantelli}
  For $P,\; P_n$ and $t$ from above,
  \[ \|P_n - P\| = \sup_{t\in \Q} \left|P_n(t) - P(t)\right| \overset{p}{\longrightarrow} 0.\]  
\end{theorem} 

\begin{proof}
  The proof, adapted from~\cite{pollard2012convergence}, consists of 5 steps. At first instance, the author clarifies that we must stablish the condition of $t\in \Q$ to avoid problems with measurability. The author later proves that the theorem is true for any $t\in \R$, but for practical purposes, we will only prove it for rationals. Another remark the author makes is that this result from the real line can be later generalized for some classes of polynomials, and we will cover more about this in section 5.

  \subsubsection*{First Symmetrization}
  In the first place, define $P_n'$ as the empirical measure obtained from an independent copy of the sample $X_1',\ldots, X_n'$ of $P$. Note that for any fixed $t$, $P_n(t)$ and $P_n'(t)$ are random variables derived from their respective samples which have:
  \[ \E P_n(t) = \E P_n'(t) = P(t),\hspace*{1em} \Var P_n(t) = \Var P_n'(t) = P(t) \] 
  We will bound the concentration of $\|P_n- P_n'\|$ first, which will later result in a bound for $\|P_n - P\|$ according to the following lemma:

  \vspace*{1em}

  For now, fix $\varepsilon > 0$, and keep in mind the values $Z = P_n - P$, $Z' = P_n' - P$, $\alpha = \frac{1}{2}\varepsilon$ and $\beta = \frac{1}{2}$.
  \begin{lemma} 
    Let ${\{Z(t)\}}_{t\in T}$ and ${\{Z'(t)\}}_{t\in T}$ be independent stochastic processes under the same set of indices $T$. Also, assume that there exist $\alpha, \beta > 0$ such that
    \[ \P\left\{\sup_{t\in T} |Z(t)| \leq \alpha \right\} \geq \beta. \ \]
    It follows that, for any $\varepsilon > 0$,
    \[  \P\left\{\sup_{t\in T} |Z(t)| > \varepsilon \right\} \leq \beta^{-1} \P\left\{\sup_{t\in T} |Z(t)-Z'(t)| > \varepsilon - \alpha \right\}. \]     
  \end{lemma}

  \begin{proof}
    Since $Z,\; Z'$ are independent, it follows from the hypothesis that for any index $\tau \in T$,
    \[ \P\{|Z'(\tau)|\leq \alpha | Z\} = \P\{|Z'(\tau)|\leq \alpha\} \geq \P \left\{ \sup_{t\in T}|Z'(t)|\leq \alpha \right\} \geq \beta.\] 
    Now, fix $\tau$ such that $|Z(\tau)| > \varepsilon$ and use the previous inequality to conclude,
    \[\everymath{\displaystyle}\arraycolsep=1.8pt\def\arraystretch{1.8}
    \begin{array}{rcl}
      \beta \cdot \P\left\{\sup_{t\in T} |Z(t)| > \varepsilon \right\} & \leq & \P\{|Z'(\tau)| \leq \alpha\} \cdot \P\{|Z(\tau)| > \varepsilon\}\\
      {\text{\scriptsize ($Z,\; Z'$ are independent)}} & = & \P\{|Z'(\tau)| \leq \alpha,\; |Z(\tau)| > \varepsilon\}\\
       & \leq & \P\{ |Z(\tau) - Z'(\tau)| > \varepsilon - \alpha \}\\
        & \leq & \P\left\{\sup_{t\in T} |Z(t)-Z'(t)| > \varepsilon - \alpha \right\}.
    \end{array} \] 
  \end{proof}
  Using Chevyshev's inequality (\ref{chebyshev}) we know that the hypothesis is satisfied for the values of $\alpha$ and $\beta$ we chose:
  \[\forall t\in T: \P\left\{ |Z'(t)| \leq \alpha \right\} = \P\{|P_n(t) - P(t)| \leq \varepsilon\} \geq \frac{1}{2} = \beta,\hspace*{1em} \text{if $n \geq 8\varepsilon^{-2}$}.\]
  Therefore, using the previous lemma, we conclude that
  \begin{equation}
    \label{gc:1} 
    \P\{\|P_n-P\| > \varepsilon\} \leq 2 \P\{\|P_n-P_n'\| > \tfrac{1}{2} \varepsilon\},\hspace*{1em} \text{if $n \geq 8\varepsilon^{-2}$}.
  \end{equation}

  \vspace*{1em}

  \subsubsection*{Second Symmetrization}
  The following trick will allow us to stop considering all of the $2n$ from the previous symmetrization, and will help us to create a simpler random variable. We will initially prove the trick for unidimensional random variables, but in chapter 4, we will generalize this proof for any kind on set on $\R^n$.

  \begin{lemma}
    Let $\sigma_1, \ldots, \sigma_n$ be Rademacher random variables, that is $\P\{\sigma_i = 1\} = \P\{\sigma_i = -1\} = 1/2$. Let $Y_i = \1_{\{ X_i' \in A\}} - \1_{\{ X_i \in A\}}$, and note that,
    \[ \P\{Y_i = x\} = \P\{\sigma_i Y_i = x\},\hspace*{1em} x \in \{-1,0,1\} \] 
  \end{lemma}

  \begin{proof} In the first place, since $X_i$ and $X'_i$ are two independent and identical copies of the same distribution, the following equality holds: 

    \[ \begin{array}{rcl}
      \P\{Y_i = 1\} & = & \P\{X_i \in A\} \P\{X'_i \not\in A\}\\
      & = &  \P\{X'_i \in A\} \P\{X_i \not\in A\}\\
      & = & \P\{Y_i = -1\}.
    \end{array} \]
  On the other hand, since $\sigma_i$ is also independent of $Y_i$, it follows that
  \[ \everymath{\displaystyle}\arraycolsep=1.8pt\def\arraystretch{1.8}
  \begin{array}{rcl}
    \P\{\sigma_i Y_i = 1\} & = & \P\{Y_i = 1, \sigma_i = 1\} + \P\{Y_i = -1, \sigma_i = -1\}\\
    & = & \P\{Y_i = 1\} \P\{\sigma_i = 1\} + \P\{Y_i = -1\} \P\{\sigma_i = 1\}\\
    & = & \frac{1}{2} \P\{Y_i = 1\} + \frac{1}{2} \P\{Y_i = 1\}\\
    & = & \P\{Y_i = 1\} = \P\{Y_i = -1\} = \P\{\sigma_i Y_i = -1\}.
  \end{array}
    \]
    Thus,
    \[  \P\{\sigma_i Y_i = \pm 1\} = \P\{Y_i = \pm 1\},\hspace*{1em}  \P\{\sigma_i Y_i = 0\} = \P\{ Y_i = 0\}. \] 
  \end{proof}
  
  It follows that since $P_n - P_n' = n^{-1} \sum_{i \leq n} Y_i$,
  \begin{equation}
    \label{gc:2} 
    \everymath{\displaystyle}\arraycolsep=1.8pt\def\arraystretch{3}
  \begin{array}{rcl}
    \P\{\|P_n - P_n'\| > \tfrac{1}{2}\varepsilon\} & = & \P\left\{\sup_{t\in \Q} \left|n^{-1}\sum_{i = 1}^n \sigma_i Y_i\right| >\tfrac{1}{2}\varepsilon \right\}\\
    & \leq & \P\left\{\sup_{t\in \Q} \left|n^{-1}\sum_{i = 1}^n \sigma_i\1_{\{ X_i < t\}}\right| >\tfrac{1}{2}\varepsilon \right\}\\
    & & + \P\left\{\sup_{t\in \Q} \left|n^{-1}\sum_{i = 1}^n \sigma_i\1_{\{ X_i' < t\}}\right| >\tfrac{1}{2}\varepsilon \right\}
  \end{array}
  \end{equation}
  \[ = 2\P\{\|P_n^\circ\| > \tfrac{1}{4} \varepsilon\}.  \] 
  where $P_n^\circ = n^{-1}\sum_{i\leq n} \sigma_i \1_{\{ X_i < t\}}$. Then, from equations $\ref{gc:1},\;\ref{gc:2}$ we conclude that for $n \geq 8\varepsilon^{-2}$,
  \[ \P\{\|P_n-P\| > \varepsilon\} \leq 4 \P\{\|P_n^\circ\|> \tfrac{1}{4}\varepsilon\}.\]

  \vspace*{1em}

  \subsubsection*{Maximal Inequality}

  \[\begin{tikzcd}
    {-\infty} & {X_{(1)}} & {X_{(2)}} & {X_{(3)}} & \cdots & {X_{(n)}} & \infty
    \arrow["{t_1}", no head, from=1-2, to=1-3]% chktex 18
    \arrow["{t_2}", no head, from=1-3, to=1-4]% chktex 18
    \arrow["{t_3}", no head, from=1-4, to=1-5]% chktex 18
    \arrow["{t_{n-1}}", no head, from=1-5, to=1-6]% chktex 18
    \arrow["{t_0}"', from=1-2, to=1-1]% chktex 18
    \arrow["{t_n}", from=1-6, to=1-7]% chktex 18
  \end{tikzcd}\]

  For any given sample $X = X_1, \ldots, X_n$, define $X_{(j)}$ as the $j$-th observation when we order the observations, and fix $t_j \in (X_{(j)}, X_{(j+1)}]$ for every $j\leq n$ as the picture above shows. Note that if $t \in (X_{(j)}, X_{(j+1)}]$, then $P_n^\circ (t) = P_n^\circ (t_j)$ because:  % chktex 9

  \[ \everymath{\displaystyle}\arraycolsep=1.8pt\def\arraystretch{1.8}
  \begin{array}{rclcl}
    P_n^\circ (t) & = & n^{-1}\sum_{i = 1}^n \sigma_i\1_{\{ X_i < t\}}, & & \hfill t \in (X_{(j)}, X_{(j+1)}] \\ % chktex 9
    & = & n^{-1}\sum_{i = j+1}^{n} \sigma_i\1_{\{ X_{(i)} < t\}} & + & n^{-1} \sum_{i = 1}^{j} \sigma_i \1_{\{ X_{(i)} < t\}}\\
    & = & n^{-1}\sum_{i = j+1}^{n} \sigma_i \cdot 1 & + &  \hfill0\hfill\\
    & = & P_n^\circ (t_j).
  \end{array}
  \]

  It follows that for some $k$, $\|P_n^\circ\| = |P_n^\circ (t_k)|$, and thus,
   \begin{equation} \label{gc:3}
    \everymath{\displaystyle}\arraycolsep=1.4pt\def\arraystretch{1.5}
    \begin{array}{rcl}
      \P\{ \|P_n^\circ\| > \tfrac{1}{4} \varepsilon \;|\; X \} & \leq & \sum_{j = 0}^n\P\{ |P_n^\circ(t_k)| > \tfrac{1}{4} \varepsilon \;|\; X \}\\
      & \leq & (n+1) \cdot \max_j \P\{ |P_n^\circ(t_k)| > \tfrac{1}{4} \varepsilon \;|\; X \} .
    \end{array} 
  \end{equation}
  \vspace*{1em}

  \subsubsection*{Exponential Bounds}

  Since for any given sample, $\sigma \1_{X_i < t} \in [-1,1]$, we can use Hoeffding's Inequality~\ref{hoeffding:bounded} to obtain the following inequality
  \[ \P \{ |P_n^\circ(t)| > \tfrac{1}{4} \varepsilon \} \leq 2 \exp\left( \frac{-2{(n\varepsilon/4)}^2}{4n}\right) = 2e^{-n\varepsilon^2/32}.\]
  We use equation $\ref{gc:3}$ to conclude
  \begin{equation} \label{gc:4}
    \P\{ \|P_n^\circ\| > \tfrac{1}{4} \varepsilon \;|\; X\} \leq 2(n+1) e^{-n\varepsilon^2/32}.
  \end{equation} 

  \vspace*{1em}

  \subsubsection*{Integration}

  Finally, applying the formula $P\{A\} = \E_X[\P\{A | X\}]$, we conclude that

  \begin{equation}
    \label{gc:5}\begin{array}{rcl}
      \P \{\|P_n-P\| > \varepsilon\} & = & \E [\P \{\|P_n-P\| > \varepsilon \;|\; X\}]\\
      & \leq & \E [8(n+1) e^{-n\varepsilon^2/32}]\\
      & = & 8(n+1) e^{-n\varepsilon^2/32}
    \end{array} 
  \end{equation}

  The Borel-Cantelli states that if the probability of a sequence of events is summable, that is \(\sum_{n = 1}^\infty \P\{A_n\} < \infty\), then
  \[ \lim_n \P(A_n) \leq \P \left\{ \bigcap_{n=1}^{\infty}\bigcup_{k = n}^{\infty} A_n\right\} = 0. \] 

  Since the inequality we obtain through the previous steps is exponential, the probabilities of the events $A_n = \{\|P_n-P\| > \varepsilon\}$ are summable:

  \[  \sum_{n = 1}^\infty \P \{\|P_n-P\| > \varepsilon\} < \infty .\] 

  Therefore, using the Borel-Cantelli lemma we conclude that

  \[ \P \{\|P_n-P\| > \varepsilon\} \to 0 \mbox{ with probability 1.} \] 
\end{proof}

In chapter 4 we will elaborate further on the details required to transform this powerful theorem in a more generalized version.

% chktex 17