\chapter{Exponential Inequalities}

Even if we are satisfied with the linear convergence rate provided by Chebyshev's inequality, there are simple ways to improve this bound. The following result will provide the idea from which the exponential inequalities derive

%% --- MGF Inequality
\begin{theorem}[MGF inequality]\label{mgf}
  Let $X_i$ be independent random variables and let $S_N := \sum_{i = 1}^N a_i X_i$. Let $\lambda > 0$ the following inequality holds,
  \[ \P\left\{  S_N \geq t\right\} \leq e^{-\lambda t}\cdot \prod_{i = 1}^N \E e^{\lambda a_i X_i} \] 

\end{theorem}

\begin{proof}
  Let $\lambda > 0$, using Markov's inequality (Theorem~\ref{markov}) we assert that since $x\mapsto e^{\lambda x}$ is a non-decreasing function,
  \[ \P\left\{  S_N \geq t\right\} = \P\left\{  e^{\lambda S_N} \geq e^{\lambda t}\right\} \leq e^{-\lambda t} \cdot \E \exp\left( \lambda \sum_{i = 1}^N a_i X_i \right). \]
  Since $X_i$ are independent, the MGF of $S_N$ is the product of MGFs of each $X_i$:
  \[\E \exp\left( \lambda \sum_{i = 1}^N a_i X_i \right) = \prod_{i = 1}^N \E e^{\lambda a_i X_i} \] 
  \[ \implies \P\left\{  S_N \geq t\right\} \leq e^{-\lambda t}\cdot \prod_{i = 1}^N \E e^{\lambda a_i X_i}. \]
\end{proof}
%% --------------------

The following two theorems are examples on how we can obtain tighter bounds than the ones provided by Chebyshev's inequality. In particular, these theorems are derived from the idea of the previous theorem and are considered as corollaries by some authors.

%% --- Chernoff's inequality
\begin{theorem}[Chernoff's inequality]\label{chernoff:bernoulli}
  Let $X_i \sim \Be(p_i)$ be independent random variables. Define $S_N = \sum_{i = 1}^{N} X_i$ and let $\mu = \E S_N$. Then, for $t > \mu$, we have
  
  \[ \P\left\{  S_N \geq t\right\} \leq  {\left(\frac{\mu}{t}\right)}^t e^{-\mu+t} .\] 
\end{theorem}

\begin{proof}
  In the first place, use Theorem~\ref{mgf} to assert that for a $\lambda > 0$ that
  \[\P\left\{  S_N \geq t\right\} \leq e^{-\lambda t}\cdot \prod_{i = 1}^N \E e^{\lambda X_i} \] 
  Now it is left to bound every $X_i$ individually. Using the inequality $1+x \leq e^x$ we obtain
  \[ \E e^{\lambda X_i} = e^{\lambda}p_i + (1-p_i) = 1 + (e^{\lambda}-1)p_i  \leq \exp(e^{\lambda}-1)e^{p_i}.\]
  Finally, we plug this inequality on the equation to conclude that
  \[e^{-\lambda t}\cdot \prod_{i = 1}^N \E e^{\lambda X_i} \leq e^{-\lambda t}\cdot \prod_{i = 1}^N \exp((e^{\lambda}-1) p_i) = e^{-\lambda t} \exp((e^{\lambda}-1) \mu). \]
  By using the substitution $\lambda = \ln (t/\mu)$ we obtain the desired result,
  \[ \P\left\{  S_N \geq t\right\} \leq {\left(\frac{\mu}{t}\right)}^t \exp{\left(\frac{\mu t}{\mu}-\mu\right)} = {\left(\frac{\mu}{t}\right)}^t e^{-\mu+t}. \]
\end{proof}
%% --------------------

Another exponential inequality that is derived using a similar technique is Hoeffding's inequality:

%% --- Hoeffding's inequality
\begin{theorem}[Hoeffding's inequality]\label{hoeffding:bernoulli}
  Let $X_1, \ldots, X_N$ be independent random variables, such that $X_i \in [a_i,b_i]$ for every $i = 1,\ldots,N$. Define $S_N = \sum_{i = 1}^{N} X_i$ and let $\mu = \E S_N$. Then, for every $t > 0$, we have
  \[ \P\left\{  S_N \geq \mu + t\right\} \leq \exp\left( \frac{-2t^2}{\sum{( a_i - b_i )}^2}\right). \]
\end{theorem}

\begin{proof}
  Since $x \mapsto e^x$ is a convex function, it follows that, for a random variable $X \in [a,b]$:
  \[ e^{\lambda X} \leq \frac{e^{\lambda a}(b-X)}{b-a} + \frac{e^{\lambda b}(X-a)}{b-a},\hspace*{1em} a \leq b. \]
  Next, take expectations on both hands of the equation to obtain:
  \[ \E e^{tX} \leq \frac{(b-\E X) \cdot e^{\lambda a}}{b-a} - \frac{(\E X -a) \cdot e^{\lambda b}}{b-a}. \]
  To simplify the expression, let $\alpha = (\E X -a)/(b-a)$, $\beta = (b-\E X)/(b-a)$ and $u = \lambda (b-a)$. Since $a < \E X < b$, it follows that $\alpha$ and $\beta$ are positive. Also, note that,
  \[ \alpha + \beta = \frac{\E X-a}{b-a} + \frac{b-\E X}{b-a} = \frac{b-a}{b-a} = 1. \] 
  Now,
  \[ \ln \E e^{\lambda X} \leq \ln (\beta e^{-\alpha u} + \alpha e^{\beta u}) = -\alpha u + \ln(\beta + \alpha e^{u}). \] 
  This function is differentiable with respect to $u$.
  \[\everymath{\displaystyle}\arraycolsep=1.8pt\def\arraystretch{1.8}
    \begin{array}{rcl}
    L(u) & = & -\alpha u + \ln(\beta + \alpha e^{u})\\
    L'(u) & = & -\alpha + \frac{\alpha}{\alpha + \beta e^{-u}}\\
    L''(u) & = & \frac{\alpha}{\alpha + \beta e^{-u}} \cdot \frac{\beta e^{-u}}{\alpha + \beta e^{-u}}.
  \end{array} \]
  Note that if $x = \frac{\alpha}{\alpha + \beta e^{-u}} \leq 1$, then $L''(u) = x(1-x) \leq \frac{1}{4}$. Remember that $\alpha + \beta = 1$. Now, by expanding the Taylor series we obtain,
  \[\everymath{\displaystyle}\arraycolsep=1.8pt\def\arraystretch{2}
  \begin{array}{rcl}
    L(u) & = & L(0) + uL'(0) + \frac{1}{2} u^2 L''(u)\\
    & = &\ln(\beta + \alpha) + u \left(-\alpha + \frac{\alpha}{\alpha + \beta}\right) + \frac{1}{2} u^2 L''(u) \\
    & = & \frac{1}{2} u^2 L''(u)\\
    &\leq & \frac{1}{8}\lambda^2 {(b-a)}^2. \tag{$\star$}
  \end{array} \]
  Finally, use the inequality from Theorem~\ref{mgf} to conclude that
  \[\everymath{\displaystyle}\arraycolsep=1.8pt\def\arraystretch{2}
    \begin{array}{rcl}
      \P\{S_N-\mu \geq t\} & \leq & e^{-\lambda t} \prod_{i = 1}^N \E e^{\lambda X_i} \\
      &\leq^{(\star)} & e^{-\lambda t} \exp\left(\frac{1}{8}t^2 \sum_{i = 1}^{N} {(b_i-a_i)}^2\right)
    \end{array}\]  

\end{proof}
%% --------------------



%% --- Hoeffding's inequality (Bernoulli)
\begin{corollary}
  Let $X_1,\ldots, X_N$ be independent random Bernoulli variables such that $X_i \sim \Be(p_i)$, then
  \[ \P\left\{ \sum_{i = 1}^N (X_i - p_i) \geq t\right\} \leq \exp\left( \frac{-2t^2}{N}\right). \] 
\end{corollary}

\begin{proof}[]

\end{proof}
%% --------------------

Returning to the coin tossing problem, we can now make a stronger assertion of the rate of convergence of a false negative classification using Hoeffding inequality:
\[\P \left\{S_N- \frac{N}{2} \geq \frac{\varepsilon}{2} N \right\} \leq \exp\left( - \varepsilon N \right). \] 

However, this raises the question of which of the previous inequalities is better for a given problem. In the previous case, we chose Hoeffding's inequality, but when dealing with any specific problem, one needs to determine the criteria for deciding whether it's more appropriate to use Chernoff, Hoeffding, or any other inequality. In the following section, we will try to identify situations where one of these inequalities is more suitable than the other.

\section{Which inequality is better?}
Let's start with a small improvement of the Chebyshev's bound for the one-sided tails

%% --- Cantelli's inequality
\begin{theorem}[Cantelli's Inequality]\label{cantelli}
  For $t > 0$, a random variable $X$ with mean $\mu = \E X$ and variance $\sigma^2 = \Var X$, we have
  \[ \P\{X-\mu \geq t\} \leq \frac{\sigma^2}{t^2 + \sigma^2}. \] 
\end{theorem}

\begin{proof}
  In the first place note that,
  \[ \P\{ Y \geq s\} \leq \P\{ Y \geq s\} + \P\{ Y \leq s\} = \P\{|Y| \geq s\} = \P\{ Y^2 \geq s^2\} \tag*{($\star$)}. \] 
  Let $u \geq 0$, define $Y = X-\mu + u$ and $s = t+u$ to obtain
  \[ \P\{X-\mu \geq t\} = \P\{X-\mu + u \geq t + u\} = \P\{Y \geq s\}. \]
  We use $(\star)$ and Markov's inequality~(\ref{markov}) on $Y^2$ to conclude,
  \[ \P\{Y \geq s\} \overset{(\star)}{\leq} \P\{ Y^2 \geq s^2\} \overset{(\ref{markov})}{\leq} \frac{\E[{(X-\mu+u)}^2]}{{(t+u)}^2}.\]
  By linearity of expectation,
  \[ \E[{(X-\mu+u)}^2] = \E[{(X-\mu)}^2] + 2u \cdot \underbrace{\E(X-\mu)}_{0} + E(u^2) = \sigma^2 + u^2. \]

  Finally, we choose an optimal $u = \frac{\sigma^2}{t}$ to conclude
  \[
    \P\{X-\mu \geq t\} \leq \frac{\sigma^2 + u^2}{{(t+u)}^2}
     = \frac{\sigma^2 + \sigma^4/t^2}{{{(t+\sigma^2/t)}^2}}
     = \frac{\sigma^2 (\tfrac{t^2+\sigma^2}{t^2})}{{(\tfrac{t^2+\sigma^2}{t})}^2}
     = \frac{\sigma^2}{t^2+\sigma^2}\]
\end{proof}
%% --------------------

On the other hand, the two-sided tail inequality, Cantelli's inequality is not always better than Chebyshev,

\begin{corollary}[Two-sided Cantelli inequality]
  \[\P\{|X-\mu| \geq t\} \leq \frac{2\sigma^2}{t^2 + \sigma^2}. \] 
\end{corollary}

In fact, this bound is only better than Chebyshev's $t^2+\sigma^2 \leq 2t^2$, or equivalently, when $\sigma^2 \leq t^2$. However, in this case both inequalities give bounds greater than 1, and thus, are useless. Therefore, we conclude that in general Chebyshev's is better for two-sided tails and Cantelli's for one-sided tails.


\section{Uniform Law of Large Numbers}

For any probability measure $P$ on the real line and $t > \in \R$, define $P_n$ as the empirical probability measure obtain from an independent sample $X_1, \ldots, X_n$ of $P$, that is:
\[ P_n(t) = n^{-1} \cdot \# {\{ X_i < t\}}_{i\leq n}.\]

From the law of large numbers we know that for a fixed $t$, $P_n(t)$ converges to $P(t)$ with probability 1. However we can formulate a stronger statement on this convergence. The first application of concentration inequalities we are going to explore is the uniform law of large numbers, which states the following:

\begin{theorem}[Glivenko-Cantelli Theorem]\label{glivenko-cantelli}
  For $P,\; P_n$ and $t$ from above,
  \[ \|P_n - P\| = \sup_{t\in \Q} \left|P_n(t) - P(t)\right| \overset{p}{\longrightarrow} 0.\]  
\end{theorem} 

\begin{proof}
  The proof, adapted from~\cite{pollard2012convergence}, consists of 5 steps. At first instance, the author clarifies that we must stablish the condition of $t\in \Q$ to avoid problems with measurability. The author later proves that the theorem is true for any $t\in \R$, but for practical purposes, we will only prove it for rationals. Another remark the author makes is that this result from the real line can be later generalized for some classes of polynomials, and we will cover more about this in section 5.

  \subsubsection*{First Symmetrization}
  In the first place, define $P_n'$ as the empirical measure obtained from an independent copy of the sample $X_1',\ldots, X_n'$ of $P$. Note that for any fixed $t$, $P_n(t)$ and $P_n'(t)$ are random variables derived from their respective samples which have:
  \[ \E P_n(t) = \E P_n'(t) = P(t),\hspace*{1em} \Var P_n(t) = \Var P_n'(t) = P(t) \] 
  We will bound the concentration of $\|P_n- P_n'\|$ first, which will later result in a bound for $\|P_n - P\|$ according to the following lemma:

  \vspace*{1em}

  For now, fix $\varepsilon > 0$, and keep in mind the values $Z = P_n - P$, $Z' = P_n' - P$, $\alpha = \frac{1}{2}\varepsilon$ and $\beta = \frac{1}{2}$.
  \begin{lemma} 
    Let ${\{Z(t)\}}_{t\in T}$ and ${\{Z'(t)\}}_{t\in T}$ be independent stochastic processes under the same set of indices $T$. Also, assume that there exist $\alpha, \beta > 0$ such that
    \[ \P\left\{\sup_{t\in T} |Z(t)| \leq \alpha \right\} \geq \beta. \ \]
    It follows that, for any $\varepsilon > 0$,
    \[  \P\left\{\sup_{t\in T} |Z(t)| > \varepsilon \right\} \leq \beta^{-1} \P\left\{\sup_{t\in T} |Z(t)-Z'(t)| > \varepsilon - \alpha \right\}. \]     
  \end{lemma}

  \begin{proof}
    Since $Z,\; Z'$ are independent, it follows from the hypothesis that for any index $\tau \in T$,
    \[ \P\{|Z'(\tau)|\leq \alpha | Z\} = \P\{|Z'(\tau)|\leq \alpha\} \geq \P \left\{ \sup_{t\in T}|Z'(t)|\leq \alpha \right\} \geq \beta.\] 
    Now, fix $\tau$ such that $|Z(\tau)| > \varepsilon$ and use the previous inequality to conclude,
    \[\everymath{\displaystyle}\arraycolsep=1.8pt\def\arraystretch{1.8}
    \begin{array}{rcl}
      \beta \cdot \P\left\{\sup_{t\in T} |Z(t)| > \varepsilon \right\} & \leq & \P\{|Z'(\tau)| \leq \alpha\} \cdot \P\{|Z(\tau)| > \varepsilon\}\\
      {\text{\scriptsize ($Z,\; Z'$ are independent)}} & = & \P\{|Z'(\tau)| \leq \alpha,\; |Z(\tau)| > \varepsilon\}\\
       & \leq & \P\{ |Z(\tau) - Z'(\tau)| > \varepsilon - \alpha \}\\
       {\text{\scriptsize \textbf{TODO:} why?}} & \leq & \P\left\{\sup_{t\in T} |Z(t)-Z'(t)| > \varepsilon - \alpha \right\}.
    \end{array} \] 
  \end{proof}

  Using Chevyshev's inequality (\ref{chebyshev}) we know that the hypothesis is satisfied for the values of $\alpha$ and $\beta$ we chose:
  \[\forall t\in T: \P\left\{ |Z'(t)| \leq \alpha \right\} = \P\{|P_n(t) - P(t)| \leq \varepsilon\} \geq \frac{1}{2} = \beta,\hspace*{1em} \text{if $n \geq 8\varepsilon^{-2}$}\]
  Therefore, using the previous lemma, we conclude that
  \[ \P\{\|P_n-P\| > \varepsilon\} \leq 2 \P\{\|P_n-P_n'\| > \tfrac{1}{2} \varepsilon\},\hspace*{1em} \text{if $n \geq 8\varepsilon^{-2}$}. \] 

  \subsubsection*{Second Symmetrization}
  Let $\sigma_1, \ldots, \sigma_n$
\end{proof}
